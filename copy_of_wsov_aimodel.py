# -*- coding: utf-8 -*-
"""Copy of wsov_AImodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XH7T7ERkgYv4BK7Sebb85CxguPrS_tFS
"""

# Cell 1: Setup and Dependencies

# Install Google API Client for YouTube access AND UPGRADE PANDAS
!pip install --upgrade google-api-python-client pandas numpy

# Download the VADER lexicon for Sentiment Analysis
import nltk
nltk.download('vader_lexicon')

print("Dependencies installed and NLTK data downloaded.")

# Cell 2: Secure API Key Loading

from google.colab import userdata

# Load keys from Colab Secrets
# NOTE: Ensure these exact names are used in the Secrets panel!
API_KEY = userdata.get('YOUTUBE_API_KEY')
GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')

if API_KEY and GEMINI_API_KEY:
    print("✅ API Keys loaded successfully from Colab Secrets.")
else:
    print("⚠️ Warning: One or both API keys are missing from Colab Secrets. Using simulated data only.")

import pandas as pd
import numpy as np
import math
import textwrap
from googleapiclient.discovery import build
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from typing import List, Dict, Any
import warnings
import time

# Suppress pandas warning related to chained assignments
try:
    warnings.filterwarnings('ignore', category=pd.SettingWithCopyWarning)
except AttributeError:
    pass

# --- 0. CONFIGURATION AND CONSTANTS ---
# Ensure these variables match the names set in Cell 2
API_KEY = globals().get('API_KEY')
GEMINI_API_KEY = globals().get('GEMINI_API_KEY')

# >>> KEYWORDS FOR MULTI-ANALYSIS <<<
PRIMARY_KEYWORD = "smart fan"
SECONDARY_KEYWORDS = ["IoT ceiling fan", "BLDC smart fan"]
ALL_KEYWORDS = [PRIMARY_KEYWORD] + SECONDARY_KEYWORDS

ALL_BRANDS = ["atomberg", "orient", "havells", "crompton", "superfan"]
TARGET_BRAND = "atomberg" # The brand for detailed comment analysis
MAX_RESULTS_PER_SEARCH = 20
MAX_COMMENTS_PER_VIDEO = 200 # Limit for comment scraping to conserve quota

# --- Metric Logic Parameters ---
IMPACT_WEIGHTS = {'views': 0.50, 'likes': 0.30, 'comments': 0.20}
PROMINENCE_WEIGHTS = {'title': 5, 'early_description': 3, 'late_description': 1}
EARLY_DESC_CHAR_LIMIT = 100
SENTIMENT_THRESHOLD = 0.05

# --- 1. DATA COLLECTION (YouTube API) ---

def fetch_youtube_data(keyword: str, max_results: int, api_key: str) -> List[Dict[str, Any]]:
    """Fetches real YouTube video data using the API key."""
    if not api_key:
        print(f"   [FATAL] YouTube API Key is empty for '{keyword}'. Skipping.")
        return []

    try:
        print(f"--- Fetching REAL data for '{keyword}' ---")
        youtube = build('youtube', 'v3', developerKey=api_key)
        search_response = youtube.search().list(q=keyword, type='video', part='id', maxResults=max_results, relevanceLanguage='en').execute()
        video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]

        if not video_ids:
             print("   [INFO] Search returned no video IDs.")
             return []

        videos_response = youtube.videos().list(part='snippet,statistics', id=','.join(video_ids)).execute()

        video_data = []
        for item in videos_response.get('items', []):
            snippet = item.get('snippet', {})
            stats = item.get('statistics', {})

            video_data.append({
                'video_id': item.get('id'),
                'title': snippet.get('title', ''),
                'snippet': snippet.get('description', '')[:100] + '...',
                'link': f"https://www.youtube.com/watch?v={item.get('id', 'N/A')}",
                'source': snippet.get('channelTitle', 'N/A'),
                'views': int(stats.get('viewCount', 0)),
                'likes': int(stats.get('likeCount', 0)),
                'comments': int(stats.get('commentCount', 0)),
                'keyword': keyword,
                'description': snippet.get('description', '')
            })
        print(f"   [SUCCESS] Fetched {len(video_data)} real videos.")
        return video_data
    except Exception as e:
        error_message = str(e)
        if "403" in error_message or "disabled" in error_message:
             print(f"   [API ERROR] The YouTube Data API v3 may be disabled or the key is invalid. Full error: {e}")
        else:
             print(f"   [API ERROR] YouTube API call failed for '{keyword}'. Check your quota or key validity: {e}")
        return []

def fetch_comments_for_sentiment(video_id: str, max_comments: int, api_key: str, brand: str) -> Dict[str, int]:
    """Fetches comments for a specific video and analyzes sentiment for a target brand."""
    comments_data = {'positive': 0, 'negative': 0, 'total': 0, 'analyzed': 0}

    try:
        youtube = build('youtube', 'v3', developerKey=api_key)
        sia = SentimentIntensityAnalyzer()
        nextPageToken = None
        comments_fetched = 0

        print(f"      [COMMENTS] Analyzing comments for video {video_id}...")

        while comments_fetched < max_comments:
            request = youtube.commentThreads().list(
                part='snippet',
                videoId=video_id,
                maxResults=min(100, max_comments - comments_fetched),
                pageToken=nextPageToken,
                textFormat='plainText'
            )
            response = request.execute()

            for item in response.get('items', []):
                comment_text = item['snippet']['topLevelComment']['snippet']['textDisplay']
                comments_data['analyzed'] += 1

                # 1. Filter: Only analyze comments that explicitly mention the target brand
                if brand.lower() in comment_text.lower():
                    score = sia.polarity_scores(comment_text)['compound']
                    comments_data['total'] += 1

                    if score > SENTIMENT_THRESHOLD:
                        comments_data['positive'] += 1
                    elif score < -SENTIMENT_THRESHOLD:
                        comments_data['negative'] += 1

            comments_fetched += len(response.get('items', []))
            nextPageToken = response.get('nextPageToken')

            if not nextPageToken:
                break

            # Pause to respect API rate limits
            time.sleep(0.1)

    except Exception as e:
        print(f"      [COMMENT API ERROR] Failed to fetch comments for {video_id}. Error: {e}")
        return {'positive': 0, 'negative': 0, 'total': 0, 'analyzed': 0}

    print(f"      [COMMENTS] Found {comments_data['total']} relevant comments mentioning {brand.capitalize()}.")
    return comments_data


# --- METRIC CALCULATION FUNCTIONS (Simplified for brevity) ---
def calculate_impact_score(video: Dict[str, Any], max_stats: Dict[str, float]) -> float:
    """Calculates the normalized and weighted Impact Score (I_i) for a video."""
    stats = ['views', 'likes', 'comments']
    score = 0
    for stat in stats:
        value = video.get(stat, 0)
        max_val = max_stats.get(stat, 1)
        if value > 0 and max_val > 0:
            log_norm_value = math.log10(value + 1) / math.log10(max_val + 1)
        else:
            log_norm_value = 0
        score += log_norm_value * IMPACT_WEIGHTS[stat]
    return score

def calculate_prominence_score(text: Dict[str, str], brand: str) -> int:
    """Calculates the Prominence Score (P_i)."""
    brand_lower = brand.lower()
    if brand_lower in text.get('title', '').lower():
        return PROMINENCE_WEIGHTS['title']
    desc = text.get('description', '')
    if len(desc) > EARLY_DESC_CHAR_LIMIT and brand_lower in desc[:EARLY_DESC_CHAR_LIMIT].lower():
        return PROMINENCE_WEIGHTS['early_description']
    if brand_lower in desc.lower():
        return PROMINENCE_WEIGHTS['late_description']
    return 0

def get_sentiment_score(text: dict, analyzer: SentimentIntensityAnalyzer) -> float:
    """Uses VADER to get the compound sentiment score for the text."""
    if not isinstance(text, dict): return 0.0
    full_text = text.get('title', '') + " " + text.get('description', '')[:500]
    return analyzer.polarity_scores(full_text)['compound']

# --- MAIN PROCESSING LOGIC ---

def process_data_and_calculate_wsov(data: List[Dict[str, Any]], brands: List[str]) -> Dict[str, pd.DataFrame]:
    """Processes video data, calculates all metrics, and runs comment analysis."""
    if not data:
        print("Error: No data to process.")
        return {}

    df = pd.DataFrame(data)

    # 1. Video-level Calculations (WSoV, I_i, P_i)
    max_stats = {'views': df['views'].max(), 'likes': df['likes'].max(), 'comments': df['comments'].max()}
    sia = SentimentIntensityAnalyzer()
    df['I_i'] = df.apply(lambda row: calculate_impact_score(row.to_dict(), max_stats), axis=1)

    brand_mentions = {brand: [] for brand in brands}
    total_market_w_score = 0

    for brand in brands:
        for index, row in df.iterrows():
            P_i = calculate_prominence_score(row.to_dict(), brand)
            V_i = 1 if P_i > 0 else 0
            I_i = row['I_i']
            W_score = P_i * V_i * I_i

            if V_i == 1:
                brand_mentions[brand].append({
                    'index': index,
                    'V_i': V_i,
                    'P_i': P_i,
                    'I_i': I_i,
                    'W_score': W_score,
                    'video_id': row['video_id']
                })
                total_market_w_score += W_score

    # 2. Sentiment/SoPV from Titles/Descriptions (For comparison and competitors)
    results_list = []
    for brand in brands:
        brand_data = brand_mentions[brand]
        total_mentions = len(brand_data)
        total_w_score = sum(item['W_score'] for item in brand_data)

        # Calculate SoPV based on Title/Description
        df['sentiment'] = df.apply(lambda row: get_sentiment_score(row.to_dict(), sia), axis=1)

        positive_mentions_desc = df.loc[df.index.isin([item['index'] for item in brand_data]), 'sentiment'].apply(lambda x: 1 if x > SENTIMENT_THRESHOLD else 0).sum()

        wsov = (total_w_score / total_market_w_score) * 100 if total_market_w_score > 0 else 0
        sopv_desc = (positive_mentions_desc / total_mentions) * 100 if total_mentions > 0 else 0

        results_list.append({
            'Brand': brand.capitalize(),
            'WSoV (%)': wsov,
            'SoPV (Title/Desc) (%)': sopv_desc,
            'Total_Mentions': total_mentions,
            'Total_W_Score': round(total_w_score, 3)
        })

    results_df = pd.DataFrame(results_list).sort_values(by='WSoV (%)', ascending=False)
    results_df['WSoV (%)'] = results_df['WSoV (%)'].round(2)
    results_df['SoPV (Title/Desc) (%)'] = results_df['SoPV (Title/Desc) (%)'].round(2)

    # 3. Targeted Comment-Based SoPV (The Fix for Atomberg)

    print("\n" + "="*80)
    print("= INITIATING TARGETED COMMENT ANALYSIS FOR ATOMBERG =")
    print("= (Based on Top 5 Impactful Videos, max 200 comments each) =")
    print("= Note: Competitors use Title/Description SoPV due to API limits =")
    print("="*80)

    target_brand_data = brand_mentions[TARGET_BRAND]
    top_atomberg_videos = sorted(target_brand_data, key=lambda x: x['W_score'], reverse=True)[:5]

    total_relevant_comments = 0
    total_positive_comments = 0
    comment_analysis_breakdown = []

    for video in top_atomberg_videos:
        video_id = video['video_id']
        video_title = df.loc[video['index'], 'title']

        comment_scores = fetch_comments_for_sentiment(video_id, MAX_COMMENTS_PER_VIDEO, API_KEY, TARGET_BRAND)

        total_relevant_comments += comment_scores['total']
        total_positive_comments += comment_scores['positive']

        comment_analysis_breakdown.append({
            'Video Index': video['index'],
            'Video Title (Snippet)': video_title[:50] + '...',
            'Total Comments Scraped': comment_scores['analyzed'],
            'Mentions Found': comment_scores['total'],
            'Positive': comment_scores['positive'],
            'Negative': comment_scores['negative'],
            'Neutral/Other': comment_scores['total'] - comment_scores['positive'] - comment_scores['negative']
        })

    comment_breakdown_df = pd.DataFrame(comment_analysis_breakdown)

    # Calculate the new, accurate SoPV
    if total_relevant_comments > 0:
        sopv_comment = (total_positive_comments / total_relevant_comments) * 100
    else:
        sopv_comment = 0.0

    # 4. Final Table Generation

    # E. WSoV Table
    wsov_table = results_df[['Brand', 'WSoV (%)', 'Total_Mentions', 'Total_W_Score']]

    # F. New SoPV Table (Comment-Based - ONLY ATOMBERG)
    sopv_table = pd.DataFrame({
        'Brand': [TARGET_BRAND.capitalize()],
        'SoPV (Comment-Based) (%)': [round(sopv_comment, 2)],
        'Total Relevant Comments': [total_relevant_comments],
        'Total Positive Comments': [total_positive_comments],
        'Total Negative Comments': [total_relevant_comments - total_positive_comments],
        'Analysis Scope': [f'Top 5 Impact Videos (max {MAX_COMMENTS_PER_VIDEO} comments each)']
    })

    # G. Combined Result for LLM
    final_results_for_llm = results_df[['Brand', 'WSoV (%)', 'Total_Mentions']].copy()

    # Add the comment-based SoPV to the target brand row
    final_results_for_llm.loc[final_results_for_llm['Brand'] == TARGET_BRAND.capitalize(), 'SoPV (%)'] = round(sopv_comment, 2)
    # Fill others with a placeholder (or the old score) but the LLM is instructed to ignore them
    final_results_for_llm.loc[final_results_for_llm['Brand'] != TARGET_BRAND.capitalize(), 'SoPV (%)'] = results_df['SoPV (Title/Desc) (%)']

    return {
        'initial_results': df[['title', 'snippet', 'link', 'source', 'likes', 'comments', 'views']].copy().set_index(df.index),
        'mentions_breakdown': pd.DataFrame([{
            'Brand': brand.capitalize(),
            'V_i (Visibility)': item['V_i'],
            'P_i (Prominence)': item['P_i'],
            'I_i (Impact)': round(item['I_i'], 3),
            'W_score': round(item['W_score'], 3),
            'Video Index': item['index']
        } for brand in brands for item in brand_mentions[brand]]).sort_values(by=['Brand', 'W_score'], ascending=[True, False]),
        'engagement_metrics': df[['views', 'likes', 'comments', 'I_i']].copy(),
        'wsov_table': wsov_table,
        'sopv_comment_table': sopv_table,
        'comment_breakdown': comment_analysis_breakdown,
        'final_results': final_results_for_llm
    }

# --- 3. LLM RECOMMENDATIONS (Gemini API Only) ---

def generate_llm_recommendations(results_df: pd.DataFrame, gemini_key: str, keywords: List[str]) -> str:
    """Synthesizes strategic recommendations using the Gemini API."""

    if not gemini_key:
        return "Recommendations could not be generated due to missing Gemini API Key."

    try:
        from google import genai

        print("\n--- 3. LLM Synthesis: Generating Strategic Recommendations via Gemini API ---")
        client = genai.Client(api_key=gemini_key)

        wsov_data_markdown = results_df.to_markdown(index=False, floatfmt=".2f")
        target_brand = TARGET_BRAND.capitalize()

        prompt = f"""
        You are a competitive market analyst creating a two-page report for the Atomberg marketing team.
        Analysis Scope: Weighted Share of Voice (WSoV) and Share of Positive Voice (SoPV) for the smart fan category in India.
        Keywords Analyzed: {', '.join(keywords)}

        Combined Quantitative Results (The Market View):
        {wsov_data_markdown}

        **CRITICAL NOTE FOR LLM:** Only the SoPV for Atomberg is based on sentiment analysis of **real YouTube comments**, which is the TRUE measure of customer satisfaction. Competitors' SoPV scores are highly inflated and must be ignored for strategic planning.

        Task: Analyze the combined data and provide a high-level strategic summary and three actionable recommendations for {target_brand}. Use the context that Atomberg's challenge is defending its SoPV by addressing 'clunky app'/'slow speed' complaints as competition increases across the 'IoT ceiling fan' and 'BLDC smart fan' keywords.

        Structure the output as follows:
        ### Strategic Summary: Category Definition & Competitive Position

        ### Actionable Recommendations for Atomberg
        1. **WSoV Expansion & Defense:** Strategy to defend the lead and expand share. (Suggest a specific, high-impact video title).
        2. **SoPV & Product Trust Strategy (Focusing on App/Speed Gap):** How to proactively address the 'clunky app' and 'slow speed' user reviews.
        3. **Niche Ownership Strategy (IoT/Home Integration):** Identify a specific, technical IoT feature where Atomberg can own the narrative.
        """

        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt
        )
        return response.text

    except Exception as e:
        return f"   [API ERROR] Gemini API call failed: {e}. Recommendations could not be generated."

# --- 4. MAIN EXECUTION ---

def run_wsov_agent():
    """Executes the entire WSoV analysis process."""
    print("="*80)
    print(f"| Competitive WSoV Analysis for: {', '.join(ALL_KEYWORDS).upper()} |")
    print("="*80)
    print(f"Tracking Brands: {', '.join([b.capitalize() for b in ALL_BRANDS])}")
    print(f"Analysis Depth (N): Top {MAX_RESULTS_PER_SEARCH} Results per keyword.")
    print("-----------------------------------------------------")

    # 1. Data Collection (YouTube API)
    all_raw_data = []
    for keyword in ALL_KEYWORDS:
        all_raw_data.extend(fetch_youtube_data(keyword, MAX_RESULTS_PER_SEARCH, API_KEY))

    if not all_raw_data:
        print("Analysis stopped because no real data could be retrieved for any keyword.")
        return

    # 2. Metric Calculation and Table Generation
    all_tables = process_data_and_calculate_wsov(all_raw_data, ALL_BRANDS)

    # 3. Print Requested Breakdown Tables
    print("\n" + "="*80)
    print("= INTERMEDIATE ANALYSIS TABLES (For Submission Transparency) =")
    print("="*80)

    # 1. Raw Search Results Table
    print("\n### 1. Raw Search Results & Snippets (N=Total Videos)")
    print(all_tables['initial_results'].to_markdown(index=True, floatfmt=".0f"))

    # 2. Engagement Table (Ii breakdown)
    print("\n### 2. Engagement Metrics (I_i Calculation - Log Normalized)")
    print(f"The I_i score weighs (Views*0.5 + Likes*0.3 + Comments*0.2) after log normalization.")
    print(all_tables['engagement_metrics'].to_markdown(index=True, floatfmt=".3f"))

    # 3. Mentions/Prominence Table (V_i, P_i, W_score breakdown)
    print("\n### 3. Mentions, Prominence (P_i), and W-Score Breakdown")
    print(f"P_i weights: Title=5, Early Description=3, Late Description=1.")
    print(all_tables['mentions_breakdown'].to_markdown(index=False, floatfmt=".3f"))

    # 4. WSoV Table
    print("\n### 4. Final Weighted Share of Voice (WSoV) Table")
    print(all_tables['wsov_table'].to_markdown(index=False, floatfmt=".2f"))

    # 5. Targeted Comment Analysis Breakdown
    print("\n### 5. Targeted Comment Analysis Breakdown (Atomberg Only)")
    print(f"Analysis Scoped to Top 5 Impactful Videos (max {MAX_COMMENTS_PER_VIDEO} comments each).")
    print(pd.DataFrame(all_tables['comment_breakdown']).to_markdown(index=False, floatfmt=".0f"))

    # 6. New Comment-Based SoPV Table
    print("\n### 6. Share of Positive Voice (SoPV) - Based on Real Customer Comments (Atomberg Only)")
    print(all_tables['sopv_comment_table'].to_markdown(index=False, floatfmt=".2f"))

    # 7. LLM Synthesis (Gemini API)
    recommendations = generate_llm_recommendations(all_tables['final_results'], GEMINI_API_KEY, ALL_KEYWORDS)

    print("\n" + "="*80)
    print("= 7. LLM STRATEGIC RECOMMENDATIONS =")
    print("= (Recommendations based on the new, accurate Comment-Based SoPV) =")
    print("="*80)
    print(recommendations)
    print("\n" + "="*80)

if __name__ == "__main__":
    run_wsov_agent()